"""
Script Title: MIR Inference Pipeline (Spanish Prompt) for Selected Models (Phi-3 and DeepSeek)
Author: Lorena Ariceta Garcia
TFM: AI-Based Diagnosis: Ally or Risk?
     An Analysis of Language Models

Description
-----------
This script evaluates two selected language models (Phi-3 and DeepSeek) on MIR-style
multiple-choice clinical questions using a Spanish instruction prompt.

For each exam JSON file found in the input directory, the script:
1) Sends each question (stem + 4 options) to a local LLM inference server (Ollama API).
2) Extracts a single numeric choice (1‚Äì4) from the model's response using regex.
3) Stores both the predicted option and the full text response.
4) Computes performance metrics (accuracy, errors, missing answers), using positional
   alignment between predictions and ground-truth entries.
5) Exports global summary results to CSV and Excel, and saves per-exam JSON outputs.

Important Note on Evaluation
----------------------------
Some exam files may contain duplicated question numbers ("numero"). In those cases,
this script compares predictions to the gold answers by list position (index),
not by the question number field.

Requirements
------------
- Python 3.x
- requests
- pandas
- Ollama (local server running at http://localhost:11434)
- Input data generated by the previous preprocessing steps (JSON with answers)

Outputs
-------
- Log file: log_prompt_es_phi3_deepseek.txt
- Per-model JSON files with predictions:
    <output_dir>/<model>/<exam_name>_<model>.json
- Global metrics:
    prompt_es_phi3_deepseek_metrics.csv
    prompt_es_phi3_deepseek_metrics.xlsx
"""

import json
import os
import re
import sys
import csv
import requests
import pandas as pd

from collections import OrderedDict, Counter


# ================================================================
# 1. Output configuration (console + log file)
# ================================================================
OUTPUT_DIRECTORY = "/home/xs1/Desktop/Lorena/results/2_models/1_prompt/3_prompt_deepseek_phi3"
os.makedirs(OUTPUT_DIRECTORY, exist_ok=True)


class DualOutput:
    """
    Utility class to mirror stdout to both terminal and a log file.
    """
    def __init__(self, path: str):
        self.terminal = sys.__stdout__
        self.log = open(path, "w", encoding="utf-8")

    def write(self, message: str) -> None:
        self.terminal.write(message)
        self.log.write(message)

    def flush(self) -> None:
        self.terminal.flush()
        self.log.flush()


# Redirect all print output to console + file
sys.stdout = DualOutput(os.path.join(OUTPUT_DIRECTORY, "log_prompt_es_phi3_deepseek.txt"))


# ================================================================
# 2. Models and Spanish prompt configuration
# ================================================================
# This experiment uses a Spanish instruction prompt and evaluates only two models.
MODELS = {
    "phi3": "phi3:instruct",
    "deepseek": "deepseek-llm",
}

SPANISH_SYSTEM_PROMPT = (
    "Eres un profesional m√©dico que debe responder una pregunta tipo examen cl√≠nico (MIR).\n"
    "Lee cuidadosamente el CONTEXTO recuperado y luego la PREGUNTA.\n"
    "Si el contexto contiene informaci√≥n √∫til y directa, util√≠zala para responder.\n"
    "Si el contexto no aporta la respuesta, usa tu conocimiento m√©dico general.\n"
    "Tu respuesta debe seguir estrictamente este formato:\n"
    "'La respuesta correcta es la n√∫mero X' (donde X es un n√∫mero del 1 al 4).\n"
    "Despu√©s, a√±ade una sola frase breve con la justificaci√≥n principal.\n"
    "No respondas con 'No estoy seguro', no proporciones varias opciones ni copies el contexto.\n"
    "Responde siempre con una √∫nica opci√≥n num√©rica (1‚Äì4) y una frase concisa.\n\n"
)


# ================================================================
# 3. Input configuration
# ================================================================
EXAMS_DIRECTORY = "results/1_data_preparation/6_json_final"
exam_files = [f for f in os.listdir(EXAMS_DIRECTORY) if f.endswith(".json")]


# ================================================================
# 4. Global results structure
# ================================================================
global_summary = {
    model_key: {
        "correct": 0,
        "wrong": 0,
        "no_answer": 0,
        "total": 0,
        "error_examples": [],
    }
    for model_key in MODELS
}


# ================================================================
# 5. Main loop: per exam file, per model
# ================================================================
for exam_filename in exam_files:
    exam_name = os.path.splitext(exam_filename)[0]
    exam_path = os.path.join(EXAMS_DIRECTORY, exam_filename)

    with open(exam_path, "r", encoding="utf-8") as f:
        gold_data = json.load(f)

    # Warn if duplicates are present in question numbering
    numbers = [q.get("numero") for q in gold_data.get("preguntas", [])]
    duplicate_count = sum(1 for _, c in Counter(numbers).items() if c > 1)
    if duplicate_count > 0:
        print(
            f"‚ö†Ô∏è {exam_filename}: detected {duplicate_count} duplicated values in 'numero' "
            f"‚Üí evaluation shows positional (index-based) comparison.\n"
        )

    for model_key, model_id in MODELS.items():
        print(f"\nüöÄ Processing exam '{exam_name}' with model: {model_key}")

        predictions_data = {"preguntas": []}

        model_output_dir = os.path.join(OUTPUT_DIRECTORY, model_key)
        os.makedirs(model_output_dir, exist_ok=True)

        # ------------------------------------------------------------
        # 5.1 Generate predictions and store them
        # ------------------------------------------------------------
        for idx, question in enumerate(gold_data.get("preguntas", []), start=1):
            # Skip non-text questions for known image-heavy exams
            if exam_filename in ["ENFERMER√çA.json", "MEDICINA.json"] and question.get("tipo") != "texto":
                continue

            prompt = SPANISH_SYSTEM_PROMPT + question.get("enunciado", "") + "\n\n"
            for opt_idx, option in enumerate(question.get("opciones", []), start=1):
                prompt += f"{opt_idx}. {option}\n"

            print(f"\nüì§ [{idx}] Sending question to {model_key}...")

            payload = {"model": model_id, "prompt": prompt, "stream": False}

            try:
                response = requests.post(
                    "http://localhost:11434/api/generate",
                    json=payload,
                    timeout=180,
                )
                response_data = response.json()
                model_text = response_data.get("response", "").strip()

                print("üß† Model response:")
                print(model_text)

                # Extract the first standalone digit 1‚Äì4
                match = re.search(r"\b([1-4])\b", model_text)
                selected = int(match.group(1)) if match else None

                # Copy original fields and append prediction + raw text
                new_question = OrderedDict()
                for key in question:
                    if key not in (model_key, f"{model_key}_texto"):
                        new_question[key] = question[key]

                new_question[model_key] = selected
                new_question[f"{model_key}_texto"] = model_text

                predictions_data["preguntas"].append(new_question)

            except requests.exceptions.Timeout:
                print("‚ùå Model timeout.")
            except Exception as exc:
                print(f"‚ùå Error on question {idx}: {exc}")

        # Save per-model per-exam JSON
        output_json_path = os.path.join(model_output_dir, f"{exam_name}_{model_key}.json")
        with open(output_json_path, "w", encoding="utf-8") as f_out:
            json.dump(predictions_data, f_out, ensure_ascii=False, indent=2)
        print(f"\n‚úÖ Saved: {output_json_path}")

        # ------------------------------------------------------------
        # 5.2 Metrics (positional comparison)
        # ------------------------------------------------------------
        print(f"\nüìä Evaluating model {model_key.upper()} on exam: {exam_name}")

        predicted_questions = predictions_data.get("preguntas", [])
        total = len(predicted_questions)

        correct = wrong = no_answer = 0
        error_examples = []

        for i, pred_q in enumerate(predicted_questions):
            pred = pred_q.get(model_key)

            gold_answer = (
                gold_data["preguntas"][i].get("respuesta_correcta")
                if i < len(gold_data.get("preguntas", []))
                else None
            )

            if pred is None:
                no_answer += 1
            elif gold_answer is None:
                # Skip if gold answer is missing
                continue
            elif pred == gold_answer:
                correct += 1
            else:
                wrong += 1
                error_examples.append(
                    {
                        "index": i + 1,
                        "predicted": pred,
                        "correct": gold_answer,
                        "stem": pred_q.get("enunciado", ""),
                    }
                )

        answered = total - no_answer
        accuracy_pct = (correct / answered * 100) if answered > 0 else 0.0

        global_summary[model_key]["correct"] += correct
        global_summary[model_key]["wrong"] += wrong
        global_summary[model_key]["no_answer"] += no_answer
        global_summary[model_key]["total"] += total
        global_summary[model_key]["error_examples"].extend(error_examples[:3])

        print("-" * 60)
        print(f"Total questions          : {total}")
        print(f"Answered by the model    : {answered}")
        print(f"Correct                 : {correct}")
        print(f"Wrong                   : {wrong}")
        print(f"No answer (None)        : {no_answer}")
        print(f"üìà Accuracy (%)          : {accuracy_pct:.2f}%")

        print("\nüîç Error examples:")
        for err in error_examples[:5]:
            print(f"  ‚û§ Question {err['index']}: predicted {err['predicted']}, correct {err['correct']}")
            print(f"    {err['stem']}")


# ================================================================
# 6. Global summary + CSV/Excel export
# ================================================================
print("\nüìäüìäüìä GLOBAL SUMMARY BY MODEL üìäüìäüìä")

csv_path = os.path.join(OUTPUT_DIRECTORY, "prompt_es_phi3_deepseek_metrics.csv")
excel_path = os.path.join(OUTPUT_DIRECTORY, "prompt_es_phi3_deepseek_metrics.xlsx")

rows = []

for model_key in MODELS:
    total = global_summary[model_key]["total"]
    correct = global_summary[model_key]["correct"]
    wrong = global_summary[model_key]["wrong"]
    no_answer = global_summary[model_key]["no_answer"]

    answered = total - no_answer
    accuracy_pct = (correct / answered * 100) if answered > 0 else 0.0

    print(f"\nüß† Model: {model_key.upper()}")
    print("-" * 50)
    print(f"Total questions          : {total}")
    print(f"Answered                 : {answered}")
    print(f"Correct                  : {correct}")
    print(f"Wrong                    : {wrong}")
    print(f"No answer (None)         : {no_answer}")
    print(f"üìà Accuracy (%)           : {accuracy_pct:.2f}%")

    rows.append(
        {
            "Model": model_key,
            "Total": total,
            "Answered": answered,
            "Correct": correct,
            "Wrong": wrong,
            "No answer": no_answer,
            "Accuracy (%)": round(accuracy_pct, 2),
        }
    )

# Save CSV
with open(csv_path, "w", newline="", encoding="utf-8") as f_csv:
    writer = csv.DictWriter(f_csv, fieldnames=rows[0].keys())
    writer.writeheader()
    writer.writerows(rows)

# Save Excel
df = pd.DataFrame(rows)
df.to_excel(excel_path, index=False)

print(f"\n‚úÖ Global results saved to:")
print(f"   ‚Ä¢ CSV  : {csv_path}")
print(f"   ‚Ä¢ Excel: {excel_path}")
print("\n‚úÖ Pipeline completed successfully.")
